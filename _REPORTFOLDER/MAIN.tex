\documentclass[11pt]{report}

\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{apacite}
\usepackage{natbib, url}
\usepackage[english, french]{babel}
\usepackage{amsfonts, amsmath, amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage{tabularx, booktabs} 
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{hyperref}
\usepackage{caption}

\setlength\parindent{0pt}

\frenchbsetup{%
    StandardItemizeEnv=true,       
    ThinSpaceInFrenchNumbers=true, 
    og=«, fg=»                     
  }


\newtheorem{theorem}{Theorème}[chapter]
\numberwithin{equation}{section}

\usepackage{Sweave}
\begin{document}
\input{MAIN-concordance}


\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        
        \Large
        \textbf{Modélisation statistique d'évènements extrêmes}
        
        \large
        \vspace{0.4cm}
        
        Application multivariée dynamique aux catastrophes naturelles de l'Océanie
 
        \vspace{3cm}
 
        \textit{Rapport soumis dans le cadre du cours ACT-2101 du} \\
        Baccalauréat en actuariat
 
        \vspace{1cm}
        
        \textit{Par}
 
        \textbf{Marc-Olivier Ricard}
        
        \vspace{1cm}
        
        \textit{Présenté à}
 
        \textbf{Marie-Pier Côté}
 
        \vfill
 
        \includegraphics[width=0.3\textwidth]{Laval.PNG}
        
        \vspace{1.5cm}
 
        École d'actuariat\\
        Université Laval\\
        
        \vspace{0.5cm}
        
        Décembre 2019
 
    \end{center}
\end{titlepage}


\tableofcontents
\cleardoublepage


\chapter*{Résumé}
\label{chap:résumé} 
\phantomsection\addcontentsline{toc}{chapter}{\nameref{chap:résumé}}


\chapter*{Introduction}
\label{chap:introduction} 
\phantomsection\addcontentsline{toc}{chapter}{\nameref{chap:introduction}}

L'objectif principal d'une analyse de valeur extrême est d'être capable de quantifier le comportement stochastique d'un processus à des niveaux exceptionnellement élevés ou bas. De plus, ce type d'analyse a souvent comme but d'estimer la probabilité de réalisation d'évènements qui sont encore plus extrêmes que n'importe quel évènement passé. La théorie des valeurs extrêmes rend ce genre d'extrapolation possible.

\chapter{Notions préliminaires}
\label{chap:notions} 

\section{Théorie classique des valeurs extrêmes}

Posons $M_n = \max\{X_1, \dots, X_n\}$, le maximum des n observations indépendantes et de distribution commune. Dans le cas où le comportement des $X_i$ serait connu, il serait facile d'obtenir le comportement exact de $M_n$, mais, en pratique, cette situation est très rare. Par contre, sous des hypothèses appropriées et pour ${n \to \infty}$, il est possible d'approximer le comportement de $M_n$ et d'obtenir une famille de modèles qui peuvent être ajustés avec les différentes valeurs observées. On appelle cette approche le paradigme des valeurs extrêmes. Il faut ensuite être en mesure d'estimer les différents paramètres du modèle, de quantifier l'incertitude, d'évaluer le modèle et finalement de maximiser l'utilisation de l'information disponible.\\

Dans cette section, on étudie le modèle qui s'intéresse au comportement statistique de $M_n$. Nous pourrions utiliser la distribution théorique:

\begin{equation}\label{eq:1.1.1}
\begin{split}
P\{M_n \le z\} &= P\{X_1 \le z, \dots, X_n \le z\} \\
              &= P\{X_1 \le z\} \times \dots \times P\{X_n \le z\} \\
              &= \{F(z)\}^n
\end{split}
\end{equation}

Par contre, en pratique, $F$ est inconnu et donc, cette approche n'est pas utile. Il serait possible d'estimer $F$ avec les valeurs observées, mais la moindre erreur dans l'estimation pourrait mener à une très grande erreur pour $F^n$. L'approche alternative est d'approximer directement $F^n$ avec seulement les valeurs extrêmes. Étant donné que la distribution de $M_n$ est dégénératrice à un certain point, nous normalisons $M_n$: 

\begin{equation}\label{eq:1.1.2}
M^*_n = \frac{M_n - b_n}{a_n}
\end{equation}

\begin{theorem}\label{theor:1.1}
S'il existe des séquences de constantes $\{a_n > 0\}$ et $\{b_n\}$ tel que $$P\Bigg\{\frac{M_n - b_n}{a_n} \le z \Bigg\}\to G(z), \ \ \  \text{quand} \ \ {n \to \infty}$$ où $G$ est une fonction de répartition non-dégénératrice. Alors, $G$ appartient à une des distributions de valeur extrême, soit les lois Gumbel, Fréchet et Weibull. Donc, peu importe la distribution ${F_X}_i$, les trois dernières lois sont les seules distributions limites pour $M^*_n$.
\end{theorem}

Malgré qu'il pourrait sembler logique de choisir une des trois distributions et d'estimer ses paramètres, cette piste possède deux faiblesses: une technique est nécessaire pour choisir la distribution la plus appropriée et dès que cette décision est prise, les inférences qui suivent présument que le bon choix a été fait. Une meilleure analyse est possible en combinant en une seule distribution les distributions Gumbel, Fréchet et Weibull:

\begin{equation}\label{eq:1.1.3}
\begin{gathered}
G(z) = \exp \Bigg\{ - \Big[ 1 +\xi\Big(\frac{z-\mu}{\sigma}\Big) \Big]^{-1/\xi}  \Bigg\}, \\
\{z: (1 + \xi(z- \mu)/\sigma) >0\},\ -\infty<\mu>\infty,\ \  \sigma>0,\ -\infty<\xi>\infty
\end{gathered}
\end{equation}

Ceci est la famille de distributions d'extremum généralisée (GEV). Comme on peut voir, le modèle possède trois paramètres: $\mu$ (paramètre de position), $\sigma$ (paramètre d'échelle), $\xi$ (paramètre de forme).\\

Si nous revenons au Théorème \ref{theor:1.1}, une autre difficulté est le fait que, en pratique, les constantes de normalisation $a_n$ et $b_n$ sont inconnus. Ce problème est facilement résolu: 

\begin{equation*}
\text{Nous savons déjà que} \ P\Bigg\{\frac{M_n - b_n}{a_n} \le z \Bigg\} \approx G(z), \ \ \text{quand} \ n\to\infty 
\end{equation*}

\begin{equation*}
\Rightarrow P\{M_n\le z\} \approx G\Bigg\{\frac{z- b_n}{a_n}\Bigg\} = G^*(z), 
\end{equation*}

où $G^*(z)$ est simplement un autre membre de la famille \textit{GEV}. Étant donné qu'en pratique, les paramètres doivent être estimés, ceci ne change rien au modèle proposé.\\

Tout ceci mène à une première méthodologie pour modéliser les valeurs extrêmes:

\begin{itemize}
\item Les données sont groupées en séquence de longueur $n$
\item Le maximum de chaque séquence est calculé
\item Une distribution \textit{GEV} est calibrée à ces maximums
\item La distribution peut être manipulée pour obtenir différentes statistiques
\end{itemize}

La distribution permet, par exemple, d'obtenir les très grands quantiles et ceux-ci sont obtenus comme ceci:
\begin{equation}\label{eq:1.1.4}
z_p =
\begin{cases}
\mu - \frac{\sigma}{\xi}\Big[1 - \{\log(1-p)\}^{-\xi}\Big], & \xi \ne 0 \\
\mu - \sigma\log\{-\log(1-p)\}, & \xi = 0
\end{cases}
\end{equation}
où:
\begin{itemize}
\item $G(z_p) = 1-p$
\item $z_p$ est le niveau de retour correspondant à la période de retour ${1}/{p}$
\item On s'attend à ce que le niveau $z_p$ soit dépassé en moyenne une fois chaque $1/p$ année.
\item Chaque année, le niveau $z_p$ est dépassé avec probabilité $p$
\end{itemize}

Nous pouvons également poser $y_p = -\log(1-p)$ pour obtenir:
\begin{equation}\label{eq:1.1.5}
z_p =
\begin{cases}
\mu - \frac{\sigma}{\xi}\Big[1 - {y_p}^{-\xi}\Big], & \xi \ne 0 \\
\mu - \sigma\log{y_p}, & \xi = 0
\end{cases}
\end{equation}
\\

Nous simplifions maintenant la notation en dénotant les maximums par $Z_1,\dots,Z_m$, on assume que ce sont des variables indépendantes d'une distribution \textit{GEV} dont il faut estimer les paramètres. À noter, que même si les $X_i$ sont dépendants, il peut être raisonnable d'assumer que les $Z_i$ sont indépendants.

La méthode la plus populaire pour l'estimation des paramètres est la méthode du maximum de vraisemblance. À noter, que si $\xi \le -0.5$,  il est fort probable qu'il sera impossible d'obtenir des estimateurs valides. Cependant, en pratique, cette situation est plutôt rare.\\

La log-vraisemblance va comme suit dans le cas où $\xi \ne 0$:

\begin{equation}\label{eq:1.1.6}
\begin{gathered}
\ell(\mu,\sigma,\xi) = -m\log\sigma - (1+{1/\xi})\sum_{i = 1}^{m}\log\Big[1+\xi\Big(\frac{z_i- \mu}{\sigma} \Big) \Big] - 
\sum_{i = 1}^{m}\Big[1+\xi\Big(\frac{z_i- \mu}{\sigma} \Big) \Big]^{-1/\xi},\\
1+\xi\Big(\frac{z_i- \mu}{\sigma}\Big) > 0,\ \ i=1,\dots,m
\end{gathered}
\end{equation}

Dans le cas où $\xi = 0$, il faut utiliser la limite Gumbel de la distribution:

\begin{equation}\label{eq:1.1.7}
\ell(\mu,\sigma) = -m\log\sigma - \sum_{i = 1}^{m}\Big(\frac{z_i- \mu}{\sigma} \Big)  -
\sum_{i = 1}^{m}\exp\Big[-\Big(\frac{z_i- \mu}{\sigma} \Big) \Big]
\end{equation}
\\

Après l'estimation des paramètres, nous pouvons estimer différents niveaux de retour:

\begin{equation}\label{eq:1.1.8}
\hat{z_p} = 
\begin{cases}
\hat\mu - \frac{\hat\sigma}{\hat\xi}\Big[1 - {y_p}^{-\hat\xi}\Big], & \hat\xi \ne 0 \\
\hat\mu - \hat\sigma\log{y_p}, & \hat\xi = 0
\end{cases}
\end{equation}

\begin{equation}\label{eq:1.1.9}
\text{Var}(\hat{z_p}) \approx \nabla z_p^T V \nabla z_p
\end{equation}
où $V$ correspond à la matrice variance-covariance de $(\hat\mu, \hat\sigma, \hat\xi)$ et

\begin{equation}\label{eq:1.1.10}
\begin{split}
\nabla z_p^T &= \Big[ \frac{\partial z_p}{\partial \mu}, \frac{\partial z_p}{\partial \sigma}, \frac{\partial z_p}{\partial \xi}\Big] \\
&= [1,\ -\xi^{-1}(1-y_p^{-\xi}),\ \sigma\xi^{-2}(1-y_p^{-\xi}) - \sigma\xi^{-1}y_p^{-\xi}\log y_p]\Bigg|_{(\hat\mu, \hat\sigma, \hat\xi)}
\end{split}
\end{equation}
\\

Même s'il est impossible de valider l'extrapolation faite par le modèle, on peut tout de même vérifier la qualité du modèle avec les données observées. Ces quatre graphiques sont utiles à cet effet:
\begin{itemize}
\item Graphique \textit{P-P}
\item Graphique \textit{Q-Q}
\item Graphique du niveau de retour
\item Histogramme des données avec la densité prédite par le modèle 
\end{itemize}


\section{Méthode par l'approche POT (\textit{Peaks over threshold})}

Un des inconvénients de la modélisation avec les maximums est qu'il y a potentiellement des données utiles qui ne sont pas utilisées étant donné que celles-ci ne sont pas le maximum de leur séquence, mais qui auraient pu être celui d'une autre séquence. La méthode présentée dans cette section fait une meilleure utilisation des données.\\

Soit $X_1, X_2,\dots$, une séquence de variables indépendantes, identiquement distribuées et avec fonction de répartition marginale $F$. Nous considérons comme évènements extrêmes ceux qui dépassent un certain seuil $u$. Le comportement stochastique d'un évènement extrême peut donc être décrit comme suit:

\begin{equation}\label{eq:1.2.1}
{\Pr{\{ X>u+y\ |\ X>u \}} = \frac{1-F(u+y)}{1- F(u)}}
\end{equation}

Si le comportement exact de $F$ était connu, la distribution de l'excès de seuil serait également connue. Cependant, en pratique, cette situation est rare et des approximations sont alors applicables lorsque $u$ est assez grand. \\

\begin{theorem}\label{theor:1.2}
Nous savons déjà que $\Pr\{ \max\{X_1,\dots,X_n\} \le z \}\approx G(z)$
où
\begin{equation*}
{G(z) = \exp \Bigg\{ - \Big[ 1 +\xi\Big(\frac{z-\mu}{\sigma}\Big) \Big]^{-1/\xi}  \Bigg\}}
\end{equation*}
Ensuite, pour $u$ assez grand, la fonction de répartition de $X-u\ |\ X>u$ est environ:
\begin{equation*}
H(y) = 1 - \Big(1+\frac{\xi y}{\tilde\sigma}\Big)^{-1/\xi},\ \{y:\ y>0\ \ \text{et}\ \ (1+ \xi y/\tilde{\sigma})>0 \} 
\end{equation*}
où
\begin{equation*}
{\tilde{\sigma} = \sigma + \xi(u-\mu)}
\end{equation*}
Il s'agit ici de la famille de distributions Pareto généralisée.
\end{theorem}

Nous avons donc comme théorème que si les maximums ont comme distribution approximative $G$, les excès de seuil ont comme distribution approximative un membre de la famille Pareto généralisée. De plus, les paramètres de $H$ sont uniquement déterminés par ceux de la distribution \textit{GEV}. $\xi$ conserve la même valeur, par exemple. La valeur de $n$ influence les paramètres de $G$, mais pas ceux de $H$.

\begin{proof}[Démonstration du théorème \ref{theor:1.2}]

\begin{equation*}
\begin{split}
F^n(z) \approx \exp \Bigg\{ - \Big[ 1 +\xi\Big(\frac{z-\mu}{\sigma}\Big) \Big]^{-1/\xi}  \Bigg\}\\
\Rightarrow n\log F(z) \approx - \Big[ 1 +\xi\Big(\frac{z-\mu}{\sigma}\Big) \Big]^{-1/\xi} 
\end{split}
\end{equation*}

Pour $n$ assez grand, un développement de Taylor donne:
\begin{equation*}
\begin{split}
\log F(z) \approx -(1-F(z))\\
\Rightarrow 1-F(u) \approx \frac{1}{n} \Big[ 1 +\xi\Big(\frac{u-\mu}{\sigma}\Big) \Big]^{-1/\xi}\\
\Rightarrow 1-F(u+y) \approx \frac{1}{n} \Big[ 1 +\xi\Big(\frac{u+y-\mu}{\sigma}\Big) \Big]^{-1/\xi}
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
\Rightarrow \Pr{\{ X>u+y\ |\ X>u \}} &\approx \frac{n^{-1}[1 + \xi(u+y-\mu)/\sigma]^{-1/\xi}}{n^{-1}[1 + \xi(u-\mu)/\sigma]^{-1/\xi}} \\
    &= \Bigg[ 1 + \frac{\xi(u+y-\mu)/\sigma}{1 + \xi(u-\mu)/\sigma}\Bigg]^{-1/\xi} \\
    &= \Bigg[1 + \frac{\xi y}{\tilde\sigma} \Bigg]^{-1/\xi},\ \ \tilde{\sigma} = \sigma + \xi(u - \mu)
\end{split}
\end{equation*}

\end{proof}


On propose le cadre suivant pour la modélisation de valeurs extrêmes: les données brutes représentent une séquence de valeurs indépendantes et identiquement distribuées $x_1,\dots,x_n$ et les valeurs extrêmes sont identifiées en définissant un seuil de grande valeur $u$. Les observations qui dépassent ce seuil sont définies par $x_{(1)},\dots,x_{(k)}$ et les excès de seuil par $y_j = x_{(j)} - u,\ j=1,\dots,k$. Ensuite, les $y_j$ sont reconnues comme des réalisations indépendantes d'une variable aléatoire dont la distribution peut être approximée par un membre de la famille Pareto généralisée.\\

Un des défis de cette démarche est le choix de la valeur de $u$, où il faut trouver une bonne balance entre la variance et le biais du modèle. La pratique standard est de choisir le plus petit $u$ possible qui respecte les hypothèses du modèle. Une première méthode est basée sur la moyenne de la distribution Pareto généralisée:

\begin{equation}\label{eq:1.2.2}
{\text{E}(Y) = \frac{\sigma}{1-\xi},\ \ \xi <1}
\end{equation}
Si la distribution est appropriée pour modéliser les excès d'un seuil $u_0$:
\begin{equation}\label{eq:1.2.3}
{\text{E}(X-u_0\ |\ X>u_0) = \frac{\sigma_{u_{0}}}{1- \xi}}
\end{equation}
où nous utilisons $\sigma_{u_{0}}$ pour le paramètre d'échelle des excès du seuil $u_0$. De plus, si la distribution est adéquate pour $u_0$, elle l'est également pour tous les seuils $u>u_0$ avec un changement approprié du paramètre d'échelle:

\begin{equation}\label{eq:1.2.4}
\begin{split}
\text{E}(X-u\ |\ X>u) &= {\frac{\sigma_u}{1-\xi}}\\
    &= {\frac{\sigma_{u_0} + \xi{(u-u_0)}}{1-\xi}}
\end{split}
\end{equation}
Donc, $\text{E}(X-u\ |\ X>u)$ est une fonction linéaire de $u$ et est également la moyenne des excès du seuil $u$. Cette moyenne peut être empiriquement approximée pour les données disponibles. Nous savons que ces approximés devraient changer linéairement avec $u$ quand la valeur de $u$ est appropriée pour le modèle Pareto généralisé. \\

Tout ceci mène finalement à la première procédure: nous considérons les points suivants:
\begin{equation}\label{eq:1.2.5}
\Bigg\{\Bigg(u,\ \frac{1}{n_u}\sum_{i=1}^{n_u}(x_{(i)} - u) \Bigg): u<x_{\text{max}}  \Bigg\}
\end{equation}

Cette séquence de points correspond au \textit{mean residual plot}. Après un seuil $u_0$ pour lequel la distribution Pareto généralisée fournie une approximation adéquate pour les excès, le graphique devrait être linéaire en $u$. Toutefois, il peut parfois être difficile d'interpréter ce type de graphique en pratique. \\

La deuxième méthode propose d'estimer le modèle pour différents seuils. Après un seuil $u_0$ pour lequel la distribution Pareto généralisée fournie une approximation adéquate pour les excès, les estimés du paramètre de forme $\xi$ devraient rester constants et les estimés de $\sigma_u$ devraient être linéaires en $u$ à moins que $\xi =0$, car $\sigma_u = \sigma_{u_0} + \xi(u - u_0)$. Nous pouvons également modifier le paramètre d'échelle de la distribution: $\sigma^* = \sigma_u - \xi u$. Avec cette paramétrisation, $\sigma^*$ et $\xi$ devraient rester constants au-delà de $u_0$. Nous pouvons donc tracer les graphiques de $\hat\sigma^*$ et $\hat\xi$ par rapport à $u$ et sélectionner $u_0$ comme la plus petite valeur of $u$ pour laquelle les estimés sont presque constants.\\ 

Après avoir choisi un seuil, les paramètres de la distribution Pareto généralisée peuvent être estimés avec la méthode du maximum de vraisemblance. Définissons $y_1,\dots,y_k$ comme les $k$ excès du seuil $u$. Pour $\xi \ne 0$, la log-vraisemblance est:
\begin{equation}\label{eq:1.2.6}
{\ell(\sigma, \xi) = -k\log\sigma - (1+1/\xi)\sum_{i=1}^{k}\log(1+\xi{y_i}/\sigma),\ \ (1+\xi{y_i}/\sigma) >0}
\end{equation}
Si $\xi =0$:
\begin{equation}\label{eq:1.2.7}
{\ell(\sigma) = -k\log\sigma - (1/\sigma)\sum_{i=1}^{k}y_i}
\end{equation}
Tout comme avec les maximums, des techniques numériques sont nécessaires pour la maximisation.\\

Pour les niveaux de retour, nous avons:
\begin{equation}\label{eq:1.2.8}
\begin{split}
{\Pr\{ X>x\ |\ X>u\} = \Big[ 1 + \xi \Big( \frac{x-u}{\sigma}\Big)\Big]^{-1/\xi}}\\
{\Rightarrow \Pr\{X>x\} = \zeta_u \Big[1 + \xi \Big( \frac{x-u}{\sigma}\Big)\Big]^{-1/\xi}}
\end{split}
\end{equation}
où $\zeta_u = \Pr\{X>u\}$. Ensuite, le niveau $x_m$ qui est dépassé en moyenne une fois chaque $m$ observations est obtenu comme suit:

\begin{equation}\label{eq:1.2.9}
\begin{split}
{\frac{1}{m} = \zeta_u \Big[1 + \xi \Big( \frac{x-u}{\sigma}\Big)\Big]^{-1/\xi}}\\
{\Rightarrow x_m = u + \frac{\sigma}{\xi} \Big[(m\zeta_u)^\xi -1\Big],\ \ x_m>u}
\end{split}
\end{equation}

Si $\xi =0$:
\begin{equation}\label{eq:1.2.10}
x_m = u + \sigma\log(m\zeta_u)
\end{equation}
\\

Il est souvent plus utile d'obtenir le niveau  qui est dépassé en moyenne une fois chaque $N$ années. S'il y a $n_y$ observations par années, le niveau de retour \textit{N-années} est défini comme suit:
\begin{equation}\label{eq:1.2.11}
z_N = u + \frac{\sigma}{\xi} \Big[(Nn_y \zeta_u)^\xi -1\Big]
\end{equation}
Si $\xi =0$:
\begin{equation}\label{eq:1.2.12}
z_N = u + \sigma\log(Nn_y\zeta_u)
\end{equation} 
\\

Pour estimer les niveaux de retour, il suffit de remplacer les paramètres par leur estimation. Pour $\xi$ et $\sigma$, il s'agit tout simplement de $\hat\xi$ et $\hat\sigma$ obtenus avec la méthode du maximum de vraisemblance et $\hat\zeta_u = k/n$, la proportion des données observées qui dépassent $u$. De plus, $\text{Var}(\hat\zeta_u) \approx \hat\zeta_u/(1- \hat\zeta_u)$, car $k \sim \text{Bin}(n,\zeta_u)$. \\

En appliquant la méthode delta:
\begin{equation}\label{eq:1.2.13}
\text{Var}(\hat{x}_m) \approx \nabla x^{\top}_m V \nabla x_m
\end{equation}
où $V$ correspond à la matrice variance-covariance de $(\hat\zeta_u,\hat\sigma, \hat\xi)$ et
\begin{equation}\label{eq:1.2.14}
\begin{split}
\nabla x^{\top}_m &= \Big[ \frac{\partial x_m}{\partial \zeta_u}, \frac{\partial x_m}{\partial \sigma}, \frac{\partial x_m}{\partial \xi}\Big] \\
&= \Big[\sigma^\xi \zeta^{\xi -1}_u,\ \xi^{-1} \{(m\zeta_u)^{\xi} -1\},\ -\sigma\xi^{-2}\{(m\zeta_u)^{\xi} -1\} + \sigma\xi^{-1}(m\zeta_u)^{\xi}\log(m\zeta_u)\Big]\Bigg|_{(\hat\zeta_u, \hat\sigma, \hat\xi)}
\end{split}
\end{equation}

De plus, pour la deuxième méthode de sélection de seuil, nous avons:
\begin{equation}\label{eq:1.2.15}
\text{Var}(\sigma^*) \approx \nabla \sigma^{*\top} V \nabla \sigma^*
\end{equation}
et 
\begin{equation}\label{eq:1.2.16}
\begin{split}
\nabla \sigma^{*\top} &= \Bigg[ \frac{\partial \sigma^*}{\partial \sigma_u}, \frac{\partial \sigma^*}{\partial \xi}\Bigg] \\
    &= [1, -u]
\end{split}
\end{equation}

Tout comme avec les maximums, ces quatre graphiques sont utiles pour la validation du modèle:

\begin{itemize}
\item Graphique \textit{P-P}
\item Graphique \textit{Q-Q}
\item Graphique du niveau de retour
\item Histogramme des données avec la densité prédite par le modèle 
\end{itemize}

Le graphique \textit{P-P} est constitué des points:
\begin{equation*}
\{ ((i/(k+1), \hat{H}(y_{(i)}));\ i=1,\dots,k\}
\end{equation*}
où
\begin{equation*}
\hat{H}(y) = 1 - \Bigg(1+\frac{\hat\xi y}{\hat\sigma}\Bigg)^{-1/\hat\xi}
\end{equation*}

Le graphique \textit{Q-Q} est constitué des points:
\begin{equation*}
\{(\hat{H}^{-1}(i/(k+1)), y_{(i)});\ i=1,\dots,k\}
\end{equation*}
où
\begin{equation*}
\hat{H}^{-1}(y) = u + \frac{\hat\sigma}{\hat\xi}\Big[y^{-\hat\xi}-1\Big]
\end{equation*}

Le graphique du niveau de retour est constitué des points:
\begin{equation*}
\{(m, \hat{x}_m)\}
\end{equation*}
où
\begin{equation*}
{\hat{x}_m = u + \frac{\hat\sigma}{\hat\xi} \Big[(m {\hat\zeta}_u)^{\hat\xi} -1\Big]}
\end{equation*}


\section{Modèles additifs généralisés et splines}
À compléter...

\section{Techniques de bootstrap}
À compléter...

\chapter{Article étudié}
\label{chap:article} 


\chapter{Application à des données réelles}
\label{chap:application} 

Pour mettre en application l'approche proposée par \textit{chavez2016extreme***}, deux jeux de données du paquetage \texttt{CASdatasets} sont utilisés. Soit \texttt{auscathist} et \texttt{nzcathist}. Ces données représentent respectivement l'historique des catastrophes naturelles pour l'Australie ainsi que pour la Nouvelle-Zélande. Les deux jeux de données contiennent également différentes statistiques de ces catastrophes. Voici une liste des variables disponibles:
\begin{itemize}
\item \texttt{Year}: l'année d'occurence de la catastrophe.
\item \texttt{Quarter}: le trimestre d'occurence de la catastrophne.
\item \texttt{Date}: la date d'occurence complète de la catastrophe.
\item \texttt{FirstDay}: la date de la première journée d'occurence de la catastrophe.
\item \texttt{LastDay}: la date de la dernière journée de la catastrophe (seulement disponible pour l'Australie).
\item \texttt{Event}: une description de la catastrophe.
\item \texttt{Type}: le type de catastrophe.
\item \texttt{Location}: une description du lieu de la catastrophe.
\item \texttt{OriginalCost}: coût original de la catastrophe en millions de \textit{AUD} ou \textit{NZD}.
\item \texttt{NormCost2011}: coût normalisé en millions de dollars de 2011 (inflation, richesse et population).
\item \texttt{NormCost2014}: coût normalisé en millions de dollars de 2014 (inflation, richesse et population).
\end{itemize}
\

Les tables \ref{tab:3.1} et \ref{tab:3.2} contiennent un résumé statistique des données australiennes:

% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Thu Nov 14 21:37:30 2019
\begin{table}[ht]
\centering
\begin{tabular}{cccccccc}
  \hline
 & Mean & Std.Dev & Min & Median & Max & N.Valid & Pct.Valid \\ 
  \hline
NormCost2011 & 254 & 589 & 2 & 66 & 4296 & 190 & 92 \\ 
  NormCost2014 & 288 & 639 & 2 & 77 & 4606 & 206 & 100 \\ 
  OriginalCost & 104 & 295 & 1 & 15 & 2388 & 206 & 100 \\ 
  Quarter & 2 & 1 & 1 & 2 & 4 & 206 & 100 \\ 
  Year & 1995 & 12 & 1967 & 1998 & 2014 & 206 & 100 \\ 
   \hline
\end{tabular}
\caption{Résumé statistique des variables numériques pour l'Australie} 
\label{tab:3.1}
\end{table}% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Thu Nov 14 21:37:30 2019
\begin{table}[ht]
\centering
\begin{tabular}{cccccc}
  \hline
 & Freq & \% Valid & \% Valid Cum. & \% Total & \% Total Cum. \\ 
  \hline
Bushfire & 26 & 13 & 13 & 13 & 13 \\ 
  Cyclone & 33 & 16 & 29 & 16 & 29 \\ 
  Earthquake & 4 & 2 & 31 & 2 & 31 \\ 
  Flood & 25 & 12 & 43 & 12 & 43 \\ 
  Flood, Storm & 27 & 13 & 56 & 13 & 56 \\ 
  Hailstorm & 33 & 16 & 72 & 16 & 72 \\ 
  Other & 3 & 1 & 73 & 1 & 73 \\ 
  Storm & 54 & 26 & 100 & 26 & 100 \\ 
  Tornado & 1 & 0 & 100 & 0 & 100 \\ 
  $<$NA$>$ & 0 &  &  & 0 & 100 \\ 
  Total & 206 & 100 & 100 & 100 & 100 \\ 
   \hline
\end{tabular}
\caption{Distribution du Type de catastrophe pour l'Australie} 
\label{tab:3.2}
\end{table}
Les tables \ref{tab:3.3} et \ref{tab:3.4} contiennent le même type de résumé pour la Nouvelle-Zélande:
% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Thu Nov 14 21:37:30 2019
\begin{table}[ht]
\centering
\begin{tabular}{cccccccc}
  \hline
 & Mean & Std.Dev & Min & Median & Max & N.Valid & Pct.Valid \\ 
  \hline
NormCost2011 & 17 & 41 & 0 & 5 & 371 & 129 & 89 \\ 
  NormCost2014 & 138 & 1437 & 0 & 6 & 17321 & 145 & 100 \\ 
  OriginalCost & 125 & 1370 & 0 & 4 & 16500 & 145 & 100 \\ 
  Quarter & 2 & 1 & 1 & 2 & 4 & 145 & 100 \\ 
  Year & 1999 & 11 & 1968 & 2001 & 2014 & 145 & 100 \\ 
   \hline
\end{tabular}
\caption{Résumé statistique des variables numériques pour la Nouvelle-Zélande} 
\label{tab:3.3}
\end{table}% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Thu Nov 14 21:37:30 2019
\begin{table}[ht]
\centering
\begin{tabular}{cccccc}
  \hline
 & Freq & \% Valid & \% Valid Cum. & \% Total & \% Total Cum. \\ 
  \hline
Cyclone & 4 & 3 & 3 & 3 & 3 \\ 
  Earthquake & 7 & 5 & 8 & 5 & 8 \\ 
  Flood & 58 & 40 & 48 & 40 & 48 \\ 
  Flood, Storm & 9 & 6 & 54 & 6 & 54 \\ 
  Hailstorm & 8 & 6 & 59 & 6 & 59 \\ 
  Other & 10 & 7 & 66 & 7 & 66 \\ 
  Power outage & 2 & 1 & 68 & 1 & 68 \\ 
  Storm & 32 & 22 & 90 & 22 & 90 \\ 
  Tornado & 11 & 8 & 97 & 8 & 97 \\ 
  Weather & 4 & 3 & 100 & 3 & 100 \\ 
  $<$NA$>$ & 0 &  &  & 0 & 100 \\ 
  Total & 145 & 100 & 100 & 100 & 100 \\ 
   \hline
\end{tabular}
\caption{Distribution du Type de catastrophe pour la Nouvelle-Zélande} 
\label{tab:3.4}
\end{table}\

Pour chaque jeu de données, une nouvelle variable du montant de catastrophe est créée à partir de la variable \texttt{NormCost2014} (\texttt{CostUS2019}). Cette nouvelle variable représente le coût ajusté au niveau du 30 juin 2019 en considérant l'indice du prix à la consommation de chaque pays et le coût est également converti en dollar américain. La table \ref{tab:3.5} contient les chiffres utilisés \footnote{https://www.rateinflation.com/consumer-price-index  \newline https://www.exchange-rates.org/Rate/AUD/USD/6-30-2019}.

% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Thu Nov 14 21:37:30 2019
\begin{table}[ht]
\centering
\begin{tabular}{cccc}
  \hline
 & IPC 2014 & IPC 2019 & Taux de change 2019 \\ 
  \hline
AUS & 105.9000 & 114.8000 & 0.7031 \\ 
  NZ & 974.7000 & 1039.0000 & 0.6722 \\ 
   \hline
\end{tabular}
\caption{Valeurs utilisées pour CostUS2019} 
\label{tab:3.5}
\end{table}\

Étant donné que les deux jeux de données sont très semblables, ceux-ci sont regroupés en un seul jeu de données et une variable \texttt{Country} est rajoutée. Étant donné le nombre limité de données disponibles et pour rendre l'analyse pertinente et possible, seulement les variables \texttt{CostUS2019}, \texttt{Year}, \texttt{Country} et \texttt{Type} sont conservées. Les figures \ref{fig:3.1}, \ref{fig:3.2}, \ref{fig:3.3} et \ref{fig:3.4} résument bien le jeu de données final utilisé pour l'analyse.


\begin{figure}
\begin{center}
\includegraphics{MAIN-plot1}
\end{center}
\caption{Densité du logarithme du montant des catastrophes}
\label{fig:3.1}
\end{figure}



\chapter*{Conclusion}
\label{chap:conclusion} 
\phantomsection\addcontentsline{toc}{chapter}{\nameref{chap:conclusion}}




%\bibliography{biblio} 

\end{document}
