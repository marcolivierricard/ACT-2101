% Article étudié
\chapter{Article étudié}
\label{chap:article}

\section{Introduction}
\label{sec:introar}
Dans le but d'aller au-delà des notions préliminaires du chapitre \ref{chap:notions}, un article en particuler a été étudié dans le cadre du projet de recherche. Plusieurs articles ont été brièvement explorés pour finalement choisir l'article qui répondait le plus aux bessoins du présent projet. L'article choisi (\cite{chavez2016extreme}) est tout de même récent, est de longueur et difficulté convenables et se penche sur un sujet spécifique intéressant en plus de fournir un implémentation en \textsf{R} de la méthode proposée. À noter que si le temps l'avait permis, \cite{thiombiano2017nonstationary} aurait également été un article intéressant à étudier. \\

L'article choisi présente une méthodologie dans laquelle la modélisation des valeurs extrêmes dépend de variables exogènes associées à la variable réponse qui est modélisée. Plus précisément, ce sont les paramètres du modèle de valeurs extrêmes choisi qui dépendront des covariables disponibles. C'est à dire que contrairement à ce qui a été vu aux sections \ref{sec:classique} et \ref{sec:pot} où les paramètres des modèles sont uniques étant donné qu'il sont obtenus par la méthode du maximum de vraisemblance avec l'ensemble des données, les auteurs proposent, ici, d'avoir des paramètres différents pour chaque niveaux de covariables. Cette approche vise à améliorer la calibration des lois aux données et du même coup rendre l'inférence plus crédible et précise. La méthode décrite propose de modéliser la fréquence de pertes avec un processus de Poisson non-homogène dont le paramètre dépend des covariables. Cette démarche implique l'utilisation des modèles additifs généralisés, des splines ainsi que la méthode du maximum de vraisemblance pénalisé. D'un autre côté, la sévérité est modélisée avec une distribution Pareto généralisée non-stationnaire dont les paramètre dépendent des variables exogènes. \footnote{L'article présente également la méthode avec la famille \textit{GEV}, mais brièvement.}. Ici, les splines de lissage ne peuvent pas être directement appliqués et l'article propose un algorithme basé sur les paramètres orthogonaux pour palier à la situation. Pour finir, les auteurs appliquent la méthodologie proposée à des données de pertes de risques opérationnels ainsi qu'à des données simulées pour valider la proposition.
\\

La méthodologie proposée dans l'article peut s'appliquer à plusieurs contextes et types de données qui respectent certains critères:
\begin{itemize}
\item Les données représentent des pertes aléatoires encourus à des temps aléatoires.
\item Le but de l'analyse est d'estimer la distribution de la perte globale à travers le temps.
\item Les données contiennent un nombre suffisant d'observations.
\item Les données contiennent des évènement (valeurs) extrêmes.
\item Les données contiennet des covariables significatives.
\end{itemize}

Il est important de noter que l'article utilise la méthodologie proposée avec des pertes, mais travailler avec des gains serait également possible. Même chose par rapport à des très grandes valeurs ou des très petites valeurs. Les critères mentionnés sont seulement générales et représentent une situation idéale. Quelqu'un pourrait très bien tenter d'appliquer la méthode même si un ou plusiers critères ne sont pas respectés et obtenir des résultats concluants. 
\\

À noter que les sections de l'article qui présentent les techniques de modélisation classiques ne seront pas discutés dans le présente section étant donné que l'ensemble de cette information peut être trouvé dans les sections \ref{sec:classique} et \ref{sec:pot}. La brève section sur l'approche dynamique avec les maximums ne sera également pas élaborée, car le focus de l'article ainsi que celui du chapitre \ref{chap:application} est vraiment l'approche \textit{POT}. 
\\

À noter également que le paramètre d'échelle $\sigma$ sera noté $\beta$ pour la suite des choses pour suivre la notation des auteurs de l'article.

\section{Approche dynamique de la théorie des valeurs extrêmes}

Comme mentionnée à la section \ref{sec:introar}, la nouvelle méthode proposée laisse les paramètres du modèle dépendent des covariables. Cette dépendance peut être paramétrique, non-paramétrique ou bien semi-paramétrique et elle peut également inclure des intéractions entre les variables. L'idée générale du modèle de dépendance entre les paramètres et les covariables peut être représenté par l'équation suivante:

\begin{equation}\label{eq:2.2.1}
g_k(\theta_k) = f_k(x) + h_k(x), \ \ k \in \{1, \dots, p \}
\end{equation}
où:
\begin{itemize}
\item $\theta$ est le vecteur des $p$ paramètres du modèle.
\item $g_k$ est une fonction de lien.
\item $f_k$ est la fonction pour les différents niveaux d'une variable catégorique.
\item $h_k$ est soit une fonction linéaire paramétrique ou bien une fonction lisse non-paramétrique de $t$.
\end{itemize}

 L'idée de dépendance entre les paramètres du modèle et des covariables a déjà élaborée dans \cite{coles2001introduction}, ce qui est réellement nouveau dans l'approche proposée est la dépendance semi-paramétrique avec les splines de lissage. \\
 
 Ensuite, le vecteur des paramètres du modèle $\theta$ peut être estimé en maximisant la log-vraisemblance pénalisée:
 \begin{equation}\label{eq:2.2.2}
 \ell(\theta; \cdot) - \sum_{k=1}^{p}\Big(\gamma_k \int_A h''_k(t)^2 \,dt\Big)
 \end{equation}
 où $ \ell(\theta; \cdot)$ est tout simplement la log-vraisemblance du modèle, \textit{POT} ici. Le terme de pénalité est une technique répandue qui a pour but d'éviter du surapprentissage des données. À noter que plus $\gamma_k$ est grand, plus la courbe obtenue est lisse et vice-versa. 
 \\
 
On assume que le nombre d'excès du seuil $u$ sélectionné suit un processus de Poisson non-homogène avec comme taux:
 \begin{equation}\label{eq:2.2.3}
 \lambda = \lambda(x, t) = \exp(f_\lambda(x) + f_\lambda(t))
  \end{equation}
  où $f_\lambda$ est une fonction pour les différents niveaux d'une variable catégorique $x$ et $h_\lambda$ est une fonction générale qui ne dépend pas de paramètres spécifiques comme c'est le cas pour $f_\lambda$. Ensuite, l'application d'un modèle additif généralisé mène à un estimé $\hat\lambda$ qui sera utile pour le reste de la modélisation. Cette étape peut être complétée avec l'aide du logiciel \textsf{R} et du paquetage \texttt{mgcv}.
  \\  
  
  L'approche est très semblable à l'équation \ref{eq:2.2.3} pour ce qui est de la modélisation de la sévérité de la perte. Cependant, une étape supplémentaire est nécessaire pour s'assurer que les procédures de calibration des paramètres $\xi$ et $\beta$ aux données convergent. Pour ce faire, il faut absolument que ces deux paramètres soient orthogonaux par rapport à l'information de Fisher. De ce fait, $\beta$ est reparamétrisé comme suit:
 \begin{equation}\label{eq:2.2.4}
 \begin{split}
  \nu = \log((1+\xi)\beta) \ \ \xi > -1
  \Rightarrow \beta = \frac{\exp(\nu)}{1+\xi}
  \end{split}
  \end{equation}
ce qui est orthognal à $\xi$. Voir \cite{cox1987parameter} pour plus de détails. En suit la log-vraisemblance reparamétrisée:
 \begin{equation}\label{eq:2.2.5}
 \ell(\theta; \cdot) \longrightarrow \ell^r(\xi, \nu; y) = \ell\Big(\xi, \frac{\exp(\nu)}{1+\xi}; y\Big)
 \end{equation}
où Y représente le vecteur des excès de seuil $u$. 
\\

Dans la même ordre d'idée que pour $\lambda$, on définit $\xi$ et $\nu$ comme suit:
\begin{equation}\label{eq:2.2.6}
 \xi = \xi(x, t) = f_\xi(x) + f_\xi(t)
\end{equation}
  
 \begin{equation}\label{eq:2.2.7}
 \nu = \nu(x, t) = f_\nu(x) + f_\nu(t)
 \end{equation}
 
  \begin{equation}\label{eq:2.2.8}
  \Rightarrow \beta = \beta(x, t) =  \frac{\exp(\nu(x,t))}{1+\xi(x,t)}
 \end{equation}

Les équations \ref{eq:2.2.6} et \ref{eq:2.2.7} sont donc celles qui seront estimées avec les excès de seuil disponibles pour être en mesure d'obtenir $\hat\xi$ et $\hat\beta$. Ces estimateurs sont donc des estimateurs basés sur les estimateurs $\hat{f}_\xi, \hat{h}_\xi, \hat{f}_\nu$ et$ \hat{h}_\nu$. Ces derniers estimateurs sont obtenus à partir des vecteurs observés $z_i = (t_i, x_i, {y_t}_{i})$ où:
\begin{itemize}
\item $i \in \{1, \dots, n\}$.
\item $0 \leq t_1 \le \cdots \le t_n \leq T$ représente les temps d'excès de seuil.
\item $x_i$ représente le vecteur des covariables.
\item ${y_t}_{i}$ représente les réalisations de la variable aléatoire ${Y_t}_{i}$.
\item ${Y_t}_{i}$ représente les excès de seuil $u$.
\end{itemize}

Contrairement à la méthode présentée pour la fréquence (équation \ref{eq:2.2.3}), un modèle additif généralisé n'est pas suffissant ici et un algorithme est donc développé pour être en mesure d'obtenir les paramètres estimés. Pour être en mesure d'ajuster des fonctions $h_\xi$ et $h_\nu$ qui sont assez lisses ont vecteurs observés $z_i$, la log-vraisemblance de l'équation \ref{eq:2.2.2} est utilisée:
 \begin{equation}\label{eq:2.2.9}
\ell^p({f}_\xi, {h}_\xi, {f}_\nu, {h}_\nu; z_1, \dots, z_n) =  \ell^r(\xi, \nu; y) - \gamma_\xi \int_{0}^{T} h''_\xi(t)^2 \,dt - \gamma_\nu\int_{0}^{T}  h''_\nu(t)^2 \,dt 
\end{equation}
où 
\begin{itemize}
\item $ \gamma_\xi,  \gamma_\nu \geq 0$ sont les paramètres de lissage. 
\item $y=({y_t}_{1}, \dots, {y_t}_{n})$.
\item $\ell^r(\xi, \nu; y) = \sum_{i=1}^{n} \ell^r(\xi_i, \nu_i; {y_t}_{i}) = \sum_{i=1}^{n} \ell^r(\xi_i,\frac{\exp(\nu_i)}{1+\xi_i} ; y_i)$
\end{itemize}


En combinant les conditions énumérés à la section \ref{sec:gam} et \cite[p.~13]{green1993nonparametric}, pour une spline naturelle cubique $h$ ayant comme noeuds $s_1, \dots, s_m$:
\begin{equation}\label{eq:2.2.10}
\int_{0}^{T} h''(t)^2dt = h^{\top} K h
\end{equation}
où 
\begin{itemize}
\item $h=({h_s}_1, \dots, {h_s}_n) = (h(s_1), \dots, h(s_n))$,
\item $K$ est une matrice symétrique de $m$ colonnes de rang $m-2$ qui dépend des noeuds $s_1, \dots, s_m$
\end{itemize}

Du coup, l'équation \ref{eq:2.2.9} peut être réécrite comme suit:
\begin{equation}\label{eq:2.2.10}
\ell^p({f}_\xi, {h}_\xi, {f}_\nu, {h}_\nu; z_1, \dots, z_n) =  \ell^r(\xi, \nu; y) - \gamma_\xi  {h_\xi}^{\top} K {h_\xi} - \gamma_\nu  {h_\nu}^{\top} K {h_\nu}
\end{equation}

C'est avec l'obtention de cette formule que devient possible le développement d'un algorithme permettant d'obtenir $\hat\xi$ et $\hat\beta$.  Des intervalles de confiance pour les paramètres sont également obtenus avec l'aide d'une technique de bootstrap. Cet algorithme est présenté à la section \ref{sec:algos}. 
\\

Après avoir obtenu les paramètres estimés du modèle utilisé et avant d'appliquer tout genre d'inférence avec le modèle obtenu, il est nécessaire d'évaluer la qualité de celui-ci. La méthode proposée ici repose sur le fait que si les excès ${Y_t}_{i}$ suivent une distribution Pareto généralisée avec comme paramètre $\xi$ et $\beta$, $R_i = 1 - G_{\xi_i, \beta_i} ({Y_t}_{i}), \ i \in \{1, \dots, n\}$ forme approximativement un échantillon aléatoire d'une loi uniforme $U(0,1)$. Ainsi, il est possible de vérifier si les valeurs $r_i$ se comportent comme des variables aléatoires indépendantes suivant une loi exponentielle standard où
\begin{equation}\label{eq:2.2.11}
r_i = -\log(1 - G_{\hat\xi_i, \hat\beta_i}({y_t}_{i})), \ \ i \in \{1, \dots, n\}
\end{equation}

Finalement, lorsque tous les paramètres sont obtenus et que l'adéquation du modèle est validé, l'inférence et plus particulièrement le calcul de mesures de risques pour des covariables et un point dans le temps en particulier est possible:
\begin{equation}\label{eq:2.2.12}
\widehat{\text{VaR}_\alpha} = u + \frac{\hat\beta}{\hat\xi} \Bigg(\Bigg( \frac{1-\alpha}{\hat\lambda/n'}\Bigg)^{-\hat\xi} -1 \Bigg)
\end{equation}

\begin{equation}\label{eq:2.2.13}
\widehat{\text{ES}_\alpha} =
\begin{cases}
\frac{\widehat{\text{VaR}_\alpha} + \hat\beta + u\hat\xi}{1-\hat\xi}, & \hat{\xi} \in (0,1) \\
\infty, & \hat\xi 	\ge 1
\end{cases}
\end{equation}
où $n' = n'(x,t)$ représente le nombre total de pertes pour les covariables $x$ et pour le temps $t$. Cependant, en pratique, il est plus utile de modéliser directement $\rho = \rho(x, t) = \lambda(x,t)/n'(x, t)$ qui représente le taux d'excès de seuil pour $x$ et $t$ avec une régression logistique. Des intervalles de confiance sont obtenus pour les mesures de risque avec une technique de bootstrap.
\\

Un dernier court point à couvrir est le choix du paramètre de lissage souvent reconnu comme le nombre de degrés de liberté. L'utilisation du critère d'information d'Akaike (AIC) est la technique proposée par les auteurs.
\begin{equation}\label{eq:2.2.14}
\text{AIC} \propto -2 \ell(\theta; \cdot) + 2\text{Df}
\end{equation}

\section{Application à des données réelles}
La présente section discute des fait saillants de l'application de la méthodologie à des données réelles faite par les auteurs de l'article. Voir \cite{chavez2016extreme} pour de plus amples détails.



\section{Discussion}

\section{Algorithmes}
\label{sec:algos}
